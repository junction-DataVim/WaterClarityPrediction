{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a3ab335",
   "metadata": {},
   "source": [
    "# Water Quality Prediction System\n",
    "\n",
    "This notebook implements a comprehensive machine learning pipeline for predicting water quality based on various chemical and physical parameters. The system uses multiple algorithms, performs hyperparameter tuning, and provides a complete analysis of water quality classification.\n",
    "\n",
    "## Dataset Features:\n",
    "- **Temperature**: Water temperature\n",
    "- **Turbidity**: Water clarity measure (cm)\n",
    "- **Dissolved Oxygen**: DO levels (mg/L)\n",
    "- **BOD**: Biological Oxygen Demand (mg/L)\n",
    "- **pH**: Water pH level\n",
    "- **Ammonia**: Ammonia concentration (mg/L)\n",
    "- **Nitrite**: Nitrite concentration (mg/L)\n",
    "\n",
    "## Target Classes:\n",
    "- **0**: Excellent water quality\n",
    "- **1**: Good water quality  \n",
    "- **2**: Poor water quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81519df",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, machine learning, and visualization. This includes scikit-learn for ML algorithms, pandas for data manipulation, and matplotlib/seaborn for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a36d9ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88601c",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "The `load_and_preprocess_data` function handles:\n",
    "- Loading CSV data from file or string\n",
    "- Converting European decimal format (comma to dot)\n",
    "- Renaming columns for better readability\n",
    "- Optional dataset balancing to handle class imbalance\n",
    "- Displaying original and balanced class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e782d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(data_string_or_file, balance_dataset=True, target_samples_per_class=1400):\n",
    "    \"\"\"Load and preprocess the water quality data with optional balancing\"\"\"\n",
    "    \n",
    "    # Read the data - modify this based on your actual data source\n",
    "    if isinstance(data_string_or_file, str) and len(data_string_or_file) < 1000:\n",
    "        # If it's a file path\n",
    "        try:\n",
    "            df = pd.read_csv(data_string_or_file)\n",
    "        except:\n",
    "            # If it's a data string\n",
    "            df = pd.read_csv(StringIO(data_string_or_file))\n",
    "    else:\n",
    "        # If it's a data string\n",
    "        df = pd.read_csv(StringIO(data_string_or_file))\n",
    "    \n",
    "    # Handle comma decimal separators (European format)\n",
    "    feature_columns = ['Temp', 'Turbidity (cm)', 'DO(mg/L)', 'BOD (mg/L)', \n",
    "                      'pH', 'Ammonia (mg L-1 )', 'Nitrite (mg L-1 )']\n",
    "    \n",
    "    for col in feature_columns:\n",
    "        # Remove quotes and replace comma with dot for decimal separator\n",
    "        df[col] = df[col].astype(str).str.replace('\"', '').str.replace(',', '.').astype(float)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df[feature_columns]\n",
    "    y = df['Water Quality']\n",
    "    \n",
    "    # Feature names for better interpretation\n",
    "    feature_names = ['Temperature', 'Turbidity', 'Dissolved_Oxygen', 'BOD', \n",
    "                    'pH', 'Ammonia', 'Nitrite']\n",
    "    X.columns = feature_names\n",
    "    \n",
    "    print(\"Original dataset distribution:\")\n",
    "    print(y.value_counts().sort_index())\n",
    "    print()\n",
    "    \n",
    "    # Balance the dataset if requested\n",
    "    if balance_dataset:\n",
    "        X_balanced, y_balanced = balance_classes(X, y, target_samples_per_class)\n",
    "        print(f\"Balanced dataset distribution (target: {target_samples_per_class} samples per class):\")\n",
    "        print(y_balanced.value_counts().sort_index())\n",
    "        print()\n",
    "        return X_balanced, y_balanced, feature_names\n",
    "    \n",
    "    return X, y, feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e6a264",
   "metadata": {},
   "source": [
    "## 3. Class Balancing Function\n",
    "\n",
    "The `balance_classes` function addresses class imbalance by:\n",
    "- Resampling each class to have equal number of samples\n",
    "- Using bootstrap sampling (with replacement) when necessary\n",
    "- Shuffling the balanced dataset\n",
    "- Maintaining data integrity while ensuring fair representation of all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38610975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(X, y, target_samples_per_class=1400):\n",
    "    \"\"\"Balance the dataset by resampling to equal number of samples per class\"\"\"\n",
    "    \n",
    "    # Combine features and target\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Separate by class\n",
    "    class_0 = df[df['Water Quality'] == 0]  # Excellent\n",
    "    class_1 = df[df['Water Quality'] == 1]  # Good  \n",
    "    class_2 = df[df['Water Quality'] == 2]  # Poor\n",
    "    \n",
    "    # Resample each class to target number\n",
    "    class_0_resampled = resample(class_0, \n",
    "                                replace=len(class_0) < target_samples_per_class,\n",
    "                                n_samples=target_samples_per_class,\n",
    "                                random_state=42)\n",
    "    \n",
    "    class_1_resampled = resample(class_1,\n",
    "                                replace=len(class_1) < target_samples_per_class,\n",
    "                                n_samples=target_samples_per_class,\n",
    "                                random_state=42)\n",
    "    \n",
    "    class_2_resampled = resample(class_2,\n",
    "                                replace=len(class_2) < target_samples_per_class,\n",
    "                                n_samples=target_samples_per_class,\n",
    "                                random_state=42)\n",
    "    \n",
    "    # Combine resampled classes\n",
    "    df_balanced = pd.concat([class_0_resampled, class_1_resampled, class_2_resampled])\n",
    "    \n",
    "    # Shuffle the balanced dataset\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_balanced = df_balanced.drop('Water Quality', axis=1)\n",
    "    y_balanced = df_balanced['Water Quality']\n",
    "    \n",
    "    return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136f75f",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Comparison\n",
    "\n",
    "The `evaluate_models` function implements comprehensive model comparison:\n",
    "- **Models tested**: Random Forest, Gradient Boosting, SVM, Logistic Regression, K-Nearest Neighbors\n",
    "- **Evaluation metrics**: Cross-validation accuracy, test accuracy, per-class F1-scores\n",
    "- **Stratified sampling**: Maintains class distribution in train/test splits\n",
    "- **Feature scaling**: Applied appropriately for distance-based algorithms\n",
    "- **Performance reporting**: Detailed classification reports for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da08b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, y):\n",
    "    \"\"\"Evaluate multiple machine learning models with proper stratification\"\"\"\n",
    "    \n",
    "    # Split the data with stratification to maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=42, \n",
    "                                                        stratify=y)\n",
    "    \n",
    "    print(f\"Training set distribution:\")\n",
    "    print(y_train.value_counts().sort_index())\n",
    "    print(f\"Test set distribution:\")\n",
    "    print(y_test.value_counts().sort_index())\n",
    "    print()\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define models to test\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "        'SVM': SVC(random_state=42, probability=True),  # Enable probability for better analysis\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"Model Evaluation Results:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Use stratified k-fold for cross-validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Use scaled data for SVM, Logistic Regression, and KNN\n",
    "        if name in ['SVM', 'Logistic Regression', 'K-Nearest Neighbors']:\n",
    "            X_train_use = X_train_scaled\n",
    "            X_test_use = X_test_scaled\n",
    "        else:\n",
    "            X_train_use = X_train\n",
    "            X_test_use = X_test\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train_use, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_use)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Cross-validation score with stratification\n",
    "        cv_scores = cross_val_score(model, X_train_use, y_train, cv=skf, scoring='accuracy')\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'scaler': scaler if name in ['SVM', 'Logistic Regression', 'K-Nearest Neighbors'] else None,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "        \n",
    "        # Class-wise performance\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, \n",
    "                                     target_names=['Excellent', 'Good', 'Poor'])\n",
    "        print(f\"  Per-class F1-scores:\")\n",
    "        for class_name in ['Excellent', 'Good', 'Poor']:\n",
    "            print(f\"    {class_name}: {report[class_name]['f1-score']:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return results, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ed483",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Optimization\n",
    "\n",
    "The `optimize_best_model` function fine-tunes the best performing model:\n",
    "- **Grid Search**: Systematic search through hyperparameter combinations\n",
    "- **Stratified Cross-Validation**: Ensures representative sampling during optimization\n",
    "- **Model-specific parameters**: Tailored parameter grids for each algorithm type\n",
    "- **Class balancing**: Includes class_weight parameter for imbalanced data handling\n",
    "- **Pipeline creation**: Combines preprocessing and model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cb41dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_best_model(X, y, best_model_name):\n",
    "    \n",
    "    # Use stratified split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42, stratify=y)\n",
    "    \n",
    "    # Use stratified k-fold for grid search\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    if best_model_name == 'Random Forest':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'class_weight': [None, 'balanced']  # Handle any remaining class imbalance\n",
    "        }\n",
    "        X_train_use = X_train\n",
    "        X_test_use = X_test\n",
    "        scaler = None\n",
    "        \n",
    "    elif best_model_name == 'Gradient Boosting':\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "        X_train_use = X_train\n",
    "        X_test_use = X_test\n",
    "        scaler = None\n",
    "        \n",
    "    else:  # For SVM, Logistic Regression, etc.\n",
    "        scaler = StandardScaler()\n",
    "        X_train_use = scaler.fit_transform(X_train)\n",
    "        X_test_use = scaler.transform(X_test)\n",
    "        \n",
    "        if best_model_name == 'SVM':\n",
    "            model = SVC(random_state=42, probability=True)\n",
    "            param_grid = {\n",
    "                'C': [0.1, 1, 10, 100],\n",
    "                'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "                'kernel': ['rbf', 'poly', 'sigmoid'],\n",
    "                'class_weight': [None, 'balanced']\n",
    "            }\n",
    "        elif best_model_name == 'Logistic Regression':\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            param_grid = {\n",
    "                'C': [0.01, 0.1, 1, 10, 100],\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'solver': ['liblinear', 'saga'],\n",
    "                'class_weight': [None, 'balanced']\n",
    "            }\n",
    "        elif best_model_name == 'K-Nearest Neighbors':\n",
    "            model = KNeighborsClassifier()\n",
    "            param_grid = {\n",
    "                'n_neighbors': [3, 5, 7, 9, 11],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "            }\n",
    "    \n",
    "    # Perform grid search with stratified CV\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=skf, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_use, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = best_model.predict(X_test_use)\n",
    "\n",
    "    # Create pipeline if scaler is needed\n",
    "    if scaler is not None:\n",
    "        best_model = Pipeline([\n",
    "            ('scaler', scaler),\n",
    "            ('classifier', best_model)\n",
    "        ])\n",
    "    \n",
    "    print(f\"Best model performance:\")\n",
    "    print(f\"Cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    print(f\"Test accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    return best_model, scaler, X_test, y_test, y_pred, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c8530",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "The `analyze_feature_importance` function provides insights into which water quality parameters are most influential:\n",
    "- **Tree-based models**: Extracts feature importance scores\n",
    "- **Ranking**: Orders features by their predictive power\n",
    "- **Interpretation**: Helps understand which chemical/physical parameters matter most for water quality prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e37366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names, model_name):\n",
    "    \"\"\"Analyze feature importance for tree-based models\"\"\"\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nFeature Importance ({model_name}):\")\n",
    "        print(\"=\" * 40)\n",
    "        for _, row in importance_df.iterrows():\n",
    "            print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(f\"\\nFeature importance not available for {model_name}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ff8c98",
   "metadata": {},
   "source": [
    "## 7. Load Dataset\n",
    "\n",
    "Load the water quality dataset from CSV file for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cdae472",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Water-Clarity-DS.csv', 'r', encoding='utf-8') as f:\n",
    "    sample_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69565059",
   "metadata": {},
   "source": [
    "## 8. Additional Imports for Model Deployment\n",
    "\n",
    "Import libraries needed for model serialization and deployment preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ff7dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f87fa0",
   "metadata": {},
   "source": [
    "## 9. Prediction Function Creation\n",
    "\n",
    "The `create_prediction_function` creates a user-friendly interface for making predictions:\n",
    "- **Input validation**: Ensures proper data formatting\n",
    "- **Preprocessing**: Applies same scaling used during training\n",
    "- **Prediction**: Returns both class prediction and probability scores\n",
    "- **User-friendly output**: Translates numeric classes to meaningful labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc87d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_function(best_model, scaler, feature_names):\n",
    "    \"\"\"Create a function for making new predictions\"\"\"\n",
    "    \n",
    "    def predict_water_quality(temp, turbidity, do, bod, ph, ammonia, nitrite):\n",
    "        \"\"\"\n",
    "        Predict water quality based on input parameters\n",
    "        \n",
    "        Parameters:\n",
    "        - temp: Temperature\n",
    "        - turbidity: Turbidity (cm)\n",
    "        - do: Dissolved Oxygen (mg/L)\n",
    "        - bod: BOD (mg/L)\n",
    "        - ph: pH\n",
    "        - ammonia: Ammonia (mg/L)\n",
    "        - nitrite: Nitrite (mg/L)\n",
    "        \n",
    "        Returns:\n",
    "        - prediction: 0 (excellent), 1 (good), 2 (poor)\n",
    "        - probability: probability for each class\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create input DataFrame with proper feature names\n",
    "        input_data = pd.DataFrame([[temp, turbidity, do, bod, ph, ammonia, nitrite]], \n",
    "                                 columns=feature_names)\n",
    "        \n",
    "        # Scale if necessary\n",
    "        if scaler is not None:\n",
    "            input_data_scaled = pd.DataFrame(scaler.transform(input_data), \n",
    "                                           columns=feature_names)\n",
    "            input_for_prediction = input_data_scaled\n",
    "        else:\n",
    "            input_for_prediction = input_data\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = best_model.predict(input_for_prediction)[0]\n",
    "        \n",
    "        # Get probabilities if available\n",
    "        if hasattr(best_model, 'predict_proba'):\n",
    "            probabilities = best_model.predict_proba(input_for_prediction)[0]\n",
    "        else:\n",
    "            probabilities = None\n",
    "        \n",
    "        quality_labels = {0: 'Excellent', 1: 'Good', 2: 'Poor'}\n",
    "        \n",
    "        return {\n",
    "            'prediction': prediction,\n",
    "            'quality': quality_labels[prediction],\n",
    "            'probabilities': probabilities\n",
    "        }\n",
    "    \n",
    "    return predict_water_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a756a4",
   "metadata": {},
   "source": [
    "## 10. Complete Machine Learning Pipeline\n",
    "\n",
    "This cell executes the complete ML pipeline:\n",
    "\n",
    "### Process Flow:\n",
    "1. **Data Loading**: Load and preprocess the water quality dataset\n",
    "2. **Class Balancing**: Balance classes to 1400 samples each\n",
    "3. **Model Comparison**: Evaluate 5 different ML algorithms\n",
    "4. **Best Model Selection**: Choose model with highest CV score\n",
    "5. **Hyperparameter Tuning**: Optimize the best model parameters\n",
    "6. **Performance Evaluation**: Generate detailed classification reports\n",
    "7. **Feature Analysis**: Analyze which parameters are most important\n",
    "8. **Model Deployment**: Save model and create prediction function\n",
    "9. **Example Prediction**: Demonstrate how to use the trained model\n",
    "\n",
    "### Output Includes:\n",
    "- Dataset statistics and class distributions\n",
    "- Model performance comparisons\n",
    "- Confusion matrix and classification metrics\n",
    "- Feature importance rankings\n",
    "- Saved model files for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45d76c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset distribution:\n",
      "Water Quality\n",
      "0    1400\n",
      "1    1400\n",
      "2    1500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Balanced dataset distribution (target: 1400 samples per class):\n",
      "Water Quality\n",
      "0    1400\n",
      "1    1400\n",
      "2    1400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset shape: (4200, 7)\n",
      "Feature names: ['Temperature', 'Turbidity', 'Dissolved_Oxygen', 'BOD', 'pH', 'Ammonia', 'Nitrite']\n",
      "Final target distribution:\n",
      "Water Quality\n",
      "0    1400\n",
      "1    1400\n",
      "2    1400\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Training set distribution:\n",
      "Water Quality\n",
      "0    1120\n",
      "1    1120\n",
      "2    1120\n",
      "Name: count, dtype: int64\n",
      "Test set distribution:\n",
      "Water Quality\n",
      "0    280\n",
      "1    280\n",
      "2    280\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Model Evaluation Results:\n",
      "============================================================\n",
      "Random Forest:\n",
      "  Test Accuracy: 0.9536\n",
      "  CV Score: 0.9583 (+/- 0.0114)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9771\n",
      "    Good: 0.9542\n",
      "    Poor: 0.9284\n",
      "\n",
      "Random Forest:\n",
      "  Test Accuracy: 0.9536\n",
      "  CV Score: 0.9583 (+/- 0.0114)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9771\n",
      "    Good: 0.9542\n",
      "    Poor: 0.9284\n",
      "\n",
      "Gradient Boosting:\n",
      "  Test Accuracy: 0.9655\n",
      "  CV Score: 0.9685 (+/- 0.0087)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9825\n",
      "    Good: 0.9670\n",
      "    Poor: 0.9458\n",
      "\n",
      "Gradient Boosting:\n",
      "  Test Accuracy: 0.9655\n",
      "  CV Score: 0.9685 (+/- 0.0087)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9825\n",
      "    Good: 0.9670\n",
      "    Poor: 0.9458\n",
      "\n",
      "SVM:\n",
      "  Test Accuracy: 0.9190\n",
      "  CV Score: 0.9241 (+/- 0.0118)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9639\n",
      "    Good: 0.9223\n",
      "    Poor: 0.8623\n",
      "\n",
      "SVM:\n",
      "  Test Accuracy: 0.9190\n",
      "  CV Score: 0.9241 (+/- 0.0118)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9639\n",
      "    Good: 0.9223\n",
      "    Poor: 0.8623\n",
      "\n",
      "Logistic Regression:\n",
      "  Test Accuracy: 0.7964\n",
      "  CV Score: 0.8045 (+/- 0.0307)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9178\n",
      "    Good: 0.8076\n",
      "    Poor: 0.6140\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "  Test Accuracy: 0.8940\n",
      "  CV Score: 0.8949 (+/- 0.0131)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9444\n",
      "    Good: 0.9091\n",
      "    Poor: 0.8110\n",
      "\n",
      "Best performing model: Gradient Boosting\n",
      "Best CV score: 0.9685\n",
      "Test accuracy: 0.9655\n",
      "\n",
      "Optimizing best model with hyperparameter tuning...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Logistic Regression:\n",
      "  Test Accuracy: 0.7964\n",
      "  CV Score: 0.8045 (+/- 0.0307)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9178\n",
      "    Good: 0.8076\n",
      "    Poor: 0.6140\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "  Test Accuracy: 0.8940\n",
      "  CV Score: 0.8949 (+/- 0.0131)\n",
      "  Per-class F1-scores:\n",
      "    Excellent: 0.9444\n",
      "    Good: 0.9091\n",
      "    Poor: 0.8110\n",
      "\n",
      "Best performing model: Gradient Boosting\n",
      "Best CV score: 0.9685\n",
      "Test accuracy: 0.9655\n",
      "\n",
      "Optimizing best model with hyperparameter tuning...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best model performance:\n",
      "Cross-validation score: 0.9699\n",
      "Test accuracy: 0.9643\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Detailed Classification Report:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Excellent       0.96      1.00      0.98       280\n",
      "        Good       0.94      0.99      0.97       280\n",
      "        Poor       0.99      0.90      0.94       280\n",
      "\n",
      "    accuracy                           0.96       840\n",
      "   macro avg       0.97      0.96      0.96       840\n",
      "weighted avg       0.97      0.96      0.96       840\n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "    Exc  Good Poor\n",
      "Exc [280   0   0]\n",
      "Good [  0 278   2]\n",
      "Poor [ 11  17 252]\n",
      "\n",
      "\n",
      "Feature Importance (Gradient Boosting):\n",
      "========================================\n",
      "Dissolved_Oxygen: 0.2691\n",
      "Nitrite: 0.2619\n",
      "Turbidity: 0.1584\n",
      "BOD: 0.1502\n",
      "Ammonia: 0.0725\n",
      "Temperature: 0.0663\n",
      "pH: 0.0216\n",
      "\n",
      "Example Prediction:\n",
      "Quality: Poor (Class: 2)\n",
      "Probabilities - Excellent: 0.029, Good: 0.029, Poor: 0.943\n",
      "\n",
      "Model is ready for deployment!\n",
      "Dataset has been balanced to 1400 samples per class\n",
      "Use predict_func(temp, turbidity, do, bod, ph, ammonia, nitrite) to make predictions\n",
      "Best model performance:\n",
      "Cross-validation score: 0.9699\n",
      "Test accuracy: 0.9643\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "Detailed Classification Report:\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Excellent       0.96      1.00      0.98       280\n",
      "        Good       0.94      0.99      0.97       280\n",
      "        Poor       0.99      0.90      0.94       280\n",
      "\n",
      "    accuracy                           0.96       840\n",
      "   macro avg       0.97      0.96      0.96       840\n",
      "weighted avg       0.97      0.96      0.96       840\n",
      "\n",
      "Confusion Matrix:\n",
      "    Predicted\n",
      "    Exc  Good Poor\n",
      "Exc [280   0   0]\n",
      "Good [  0 278   2]\n",
      "Poor [ 11  17 252]\n",
      "\n",
      "\n",
      "Feature Importance (Gradient Boosting):\n",
      "========================================\n",
      "Dissolved_Oxygen: 0.2691\n",
      "Nitrite: 0.2619\n",
      "Turbidity: 0.1584\n",
      "BOD: 0.1502\n",
      "Ammonia: 0.0725\n",
      "Temperature: 0.0663\n",
      "pH: 0.0216\n",
      "\n",
      "Example Prediction:\n",
      "Quality: Poor (Class: 2)\n",
      "Probabilities - Excellent: 0.029, Good: 0.029, Poor: 0.943\n",
      "\n",
      "Model is ready for deployment!\n",
      "Dataset has been balanced to 1400 samples per class\n",
      "Use predict_func(temp, turbidity, do, bod, ph, ammonia, nitrite) to make predictions\n",
      "\n",
      "Model saved to water_quality_model.pkl\n",
      "Feature names and class labels saved\n",
      "\n",
      "Model saved to water_quality_model.pkl\n",
      "Feature names and class labels saved\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data with balancing\n",
    "    # Replace 'your_data_file.csv' with your actual file path or data string\n",
    "    X, y, feature_names = load_and_preprocess_data(sample_data, balance_dataset=True, target_samples_per_class=1400)\n",
    "    \n",
    "    print(\"Dataset shape:\", X.shape)\n",
    "    print(\"Feature names:\", feature_names)\n",
    "    print(\"Final target distribution:\")\n",
    "    print(y.value_counts().sort_index())\n",
    "    print()\n",
    "    \n",
    "    # Evaluate models\n",
    "    results, X_test, y_test = evaluate_models(X, y)\n",
    "    \n",
    "    # Find best model based on cross-validation score for better generalization\n",
    "    best_model_name = max(results.keys(), key=lambda x: results[x]['cv_mean'])\n",
    "    print(f\"Best performing model: {best_model_name}\")\n",
    "    print(f\"Best CV score: {results[best_model_name]['cv_mean']:.4f}\")\n",
    "    print(f\"Test accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Optimize best model\n",
    "    print(\"Optimizing best model with hyperparameter tuning...\")\n",
    "    best_model, scaler, X_test_opt, y_test_opt, y_pred_opt, best_params = optimize_best_model(X, y, best_model_name)\n",
    "    \n",
    "    print(\"Best parameters:\", best_params)\n",
    "    print()\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(classification_report(y_test_opt, y_pred_opt, target_names=['Excellent', 'Good', 'Poor']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test_opt, y_pred_opt)\n",
    "    print(\"    Predicted\")\n",
    "    print(\"    Exc  Good Poor\")\n",
    "    print(\"Exc\", cm[0])\n",
    "    print(\"Good\", cm[1])  \n",
    "    print(\"Poor\", cm[2])\n",
    "    print()\n",
    "    \n",
    "    # Feature importance analysis\n",
    "    importance_df = analyze_feature_importance(best_model, feature_names, best_model_name)\n",
    "    \n",
    "    # Create prediction function\n",
    "    predict_func = create_prediction_function(best_model, scaler, feature_names)\n",
    "    \n",
    "    # Example prediction\n",
    "    print(\"\\nExample Prediction:\")\n",
    "    example_result = predict_func(67.45, 10.13, 0.208, 7.474, 4.752, 0.286, 4.355)\n",
    "    print(f\"Quality: {example_result['quality']} (Class: {example_result['prediction']})\")\n",
    "    if example_result['probabilities'] is not None:\n",
    "        print(\"Probabilities - Excellent: {:.3f}, Good: {:.3f}, Poor: {:.3f}\".format(\n",
    "            example_result['probabilities'][0], \n",
    "            example_result['probabilities'][1], \n",
    "            example_result['probabilities'][2]))\n",
    "    \n",
    "    print(\"\\nModel is ready for deployment!\")\n",
    "    print(\"Dataset has been balanced to 1400 samples per class\")\n",
    "    print(\"Use predict_func(temp, turbidity, do, bod, ph, ammonia, nitrite) to make predictions\")\n",
    "\n",
    "\n",
    "     # ===== NEW: Save artifacts for deployment =====\n",
    "    # 1. Save model\n",
    "    model_filename = \"water_quality_model.pkl\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # 2. Save feature names\n",
    "    with open('feature_names.json', 'w') as f:\n",
    "        json.dump(feature_names, f)\n",
    "    \n",
    "    # 3. Save class labels mapping\n",
    "    class_labels = {0: 'Excellent', 1: 'Good', 2: 'Poor'}\n",
    "    with open('class_labels.json', 'w') as f:\n",
    "        json.dump(class_labels, f)\n",
    "\n",
    "    print(f\"\\nModel saved to {model_filename}\")\n",
    "    print(\"Feature names and class labels saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5cf47e",
   "metadata": {},
   "source": [
    "## 11. Results and Model Deployment\n",
    "\n",
    "This section is for additional analysis, visualizations, or testing of the deployed model. The trained model has been saved to disk along with feature names and class labels for easy deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32067fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
